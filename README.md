# Chatbot RAG Local con DeepSeek-R1

Sistema de recuperaciÃ³n y generaciÃ³n aumentada (RAG) completamente local usando DeepSeek-R1 via Ollama.

## CaracterÃ­sticas

- âœ… 100% Local (sin APIs externas)
- ðŸ“„ Soporta PDF, Word (.docx), TXT
- ðŸ§  Embeddings de alta calidad (all-mpnet-base-v2)
- ðŸ’¾ Base de datos vectorial persistente (ChromaDB)
- ðŸš€ Interfaz CLI intuitiva

## Requisitos

- Python 3.10+
- Ollama instalado con modelo `deepseek-r1:1.5b`

## InstalaciÃ³n

1. **Clonar repositorio**
```bash
git clone <tu-repo>
cd chatbot-rag
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar .env**
```bash
# El archivo .env ya estÃ¡ configurado por defecto
# Ajusta los valores si es necesario
```

5. **Crear directorios necesarios**
```bash
mkdir -p data/documents data/vectorstore
```

## Uso

### 1. Iniciar el sistema
```bash
python main.py
```

### 2. Indexar documentos
Coloca tus documentos (PDF, DOCX, TXT) en `data/documents/` y ejecuta:
```
/index
```

O especifica un directorio:
```
/index /ruta/a/tus/documentos
```

### 3. Hacer preguntas
Simplemente escribe tu pregunta:
```
Â¿QuÃ© dice el documento sobre...?
```

### 4. Comandos disponibles
- `/index [ruta]` - Indexar documentos
- `/stats` - Ver estadÃ­sticas
- `/clear` - Limpiar Ã­ndice
- `/help` - Mostrar ayuda
- `/exit` - Salir

## Estructura del Proyecto
```
chatbot-rag/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # ConfiguraciÃ³n centralizada
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ document_loader.py   # Carga de documentos
â”‚   â”œâ”€â”€ text_processor.py    # DivisiÃ³n en chunks
â”‚   â”œâ”€â”€ embeddings.py        # GeneraciÃ³n de embeddings
â”‚   â”œâ”€â”€ vector_store.py      # ChromaDB
â”‚   â”œâ”€â”€ llm_client.py        # Cliente Ollama
â”‚   â””â”€â”€ rag_engine.py        # Motor principal
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.py           # Utilidades
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/           # Tus documentos aquÃ­
â”‚   â””â”€â”€ vectorstore/         # Base de datos (auto-generado)
â””â”€â”€ main.py                  # CLI principal
```

## ConfiguraciÃ³n

Edita `.env` para ajustar:
- TamaÃ±o de chunks
- NÃºmero de resultados
- Temperatura del modelo
- Rutas personalizadas

## Troubleshooting

**Error: Modelo no encontrado**
```bash
ollama pull deepseek-r1:1.5b
```

**Error: Ollama no responde**
```bash
# AsegÃºrate de que Ollama estÃ© corriendo
ollama serve
```

**Embeddings muy lentos**
- Primera vez: Descarga ~420MB (normal)
- DespuÃ©s: DeberÃ­a ser rÃ¡pido



## Licencia

Uso personal y educativo